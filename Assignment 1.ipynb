{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "def module(*args):        \n",
    "    if isinstance(args[0], list):        \n",
    "        args = args[0]        \n",
    "    else:        \n",
    "        args = list(args)        \n",
    "    (output, error) = subprocess.Popen(['/usr/bin/modulecmd', 'python'] + args, stdout=subprocess.PIPE).communicate()\n",
    "    exec(output)\n",
    "    \n",
    "module('load', 'apps/java/jdk1.8.0_102/binary')    \n",
    "os.environ['PYSPARK_PYTHON'] = os.environ['HOME'] + '/.conda/envs/jupyter-spark/bin/python'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignment Question 1A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as funcs\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark Session for Question 1\n",
    "spark = SparkSession.builder \\\n",
    "    .master('local[2]') \\\n",
    "    .appName('Question 1') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel('WARN')\n",
    "\n",
    "# Read in the NASA HTTP request file, SPARK can unzip .gz files automatically\n",
    "logFile = spark.read.text('Data/NASA_access_log_Jul95.gz').cache()\n",
    "\n",
    "# Extract the relevant data from the logFile using Regular Expressions\n",
    "# In this assignment we do not need the full request path hence why it is stemmed\n",
    "dataFrame = logFile.select(funcs.regexp_extract('value', r'^.*\\[(\\d\\d/\\w{3}/\\d{4}:\\d{2}:\\d{2}:\\d{2} -\\d{4})]', 1).alias('date'), \n",
    "                           funcs.regexp_extract('value', r'^.+\\/([^\\/]+(?=H))', 1).alias('request'))\n",
    "\n",
    "# Convert the timestamps from dd/MMM/yyyy:HH:mm:ss Z into standard GMT form\n",
    "dataFrameWithTimeStamps = dataFrame.withColumn('date', funcs.date_format(funcs.to_timestamp('date', 'dd/MMM/yyyy:HH:mm:ss Z'), 'yyyy-MM-dd HH:mm:ss Z'))\n",
    "\n",
    "# Now we have a GMT form timestamp we can find out the day of the week for the timestamp\n",
    "# using the dayOfweek function, this returns the day as an integer ie sunday = 1, saturday = 7\n",
    "days = dataFrameWithTimeStamps.select('date', funcs.dayofweek('date'))\n",
    "# Group the Data Frame by the days of week integer and return the count for each day\n",
    "daysCount = days.groupBy('dayOfweek(date)').count().na.drop().sort('dayOfweek(date)')\n",
    "\n",
    "# Init an array of day names to ease readability\n",
    "names = ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']\n",
    "# Init an array of the number of each day in July 1995, ie Sunday, Monday and Saturday all\n",
    "# occur 5 times in that month\n",
    "frequency = [5, 5, 4, 4, 4, 4, 5]\n",
    "\n",
    "# Init a blank array to store the average counts and ease plotting\n",
    "averageCounts = []\n",
    "\n",
    "# Iterate through each day and print the average requests per day\n",
    "for row in daysCount.collect():\n",
    "    averageCounts.append(int(row['count']) / frequency[int(row['dayOfweek(date)']) - 1 ])\n",
    "    print(names[int(row['dayOfweek(date)']) - 1 ] + ': ' + str(round(int(row['count']) / frequency[int(row['dayOfweek(date)']) - 1 ])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignment Question 1B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.bar(np.arange(len(names)), averageCounts, align = 'center')\n",
    "plt.xticks(np.arange(len(names)), names, rotation = 45)\n",
    "plt.ylabel('Average Number Of Requests')\n",
    "plt.xlabel('Day of Week')\n",
    "plt.title('Average Number of HTTP Requests Per Weekday')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The trend for the number of requests per day is that there is on average half the number of requests on days in the weekend that on days during the working week, with the number of requests increasing up until a peak on Thursday. This is expected as many of the requests could be from people in work or at school, so during the weekend they would not be making requests and might leave early on Fridays. It's worth noting that there are five Mondays, Saturdays and Sundays in July 1995, this does not affect the average for these as much as expected and there is still a clear drop in requests for days in the Weekend.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignment Question 1C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the main Data Frame to return rows where the 'request' only contains gifs as required \n",
    "# Group all requests of the same gif together and count how many\n",
    "# Sort the Data Frame of gifs to descending order of count, to give the 20 most requested\n",
    "gifs = dataFrame.select(funcs.col('request')).where(funcs.col('request').contains('.gif')).groupBy('request').count()\n",
    "sortedGifs = gifs.sort('count', ascending = False).show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Assignment Question 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init blank arrays for counts and the gifs requested to ease plotting\n",
    "reqCounts = []\n",
    "gifRequests = []\n",
    "\n",
    "# Iterate through the first 20 rows of the requested gifs Data Frame\n",
    "# Add the seperated row data to corresponding array\n",
    "for row in gifs.sort('count', ascending = False).collect()[:20]:\n",
    "    reqCounts.append(int(row['count']))\n",
    "    gifRequests.append(row['request'])\n",
    "\n",
    "plt.bar(np.arange(len(gifRequests)), reqCounts, align = 'center')\n",
    "plt.xticks(np.arange(len(gifRequests)), gifRequests, rotation = 90)\n",
    "plt.ylabel('Number of Requests')\n",
    "plt.xlabel('Name of Requested .gif File')\n",
    "plt.title('Top 20 Most Requested .gif files for July 1995')\n",
    "plt.show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common .gif requests are small logos these are most likely to be from webpages; hence the reason why NASA-logosmall.gif is the most requested gif, as this appears on the most NASA webpages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignment Question 2A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Create a Spark Session for Question 2\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master('local[2]') \\\n",
    "    .appName('Question 2') \\\n",
    "    .config(\"spark.local.dir\",\"/fastdata/acp18dck\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel('WARN')\n",
    "\n",
    "# Define the Schema for the Data Frame \n",
    "schema = StructType([\n",
    "    StructField('userId', IntegerType()),\n",
    "    StructField('movieId', IntegerType()),\n",
    "    StructField('rating', FloatType()),\n",
    "    StructField('timestamp', IntegerType())\n",
    "])\n",
    "\n",
    "# Read the ratings csv file and format into pre-defined schema\n",
    "ratingsDataFrame = spark.read.format('csv').schema(schema).option('header', 'true').load('Data/ml-20m/ratings.csv')\n",
    "\n",
    "# Specify number of folds required\n",
    "numberOfFolds = 5\n",
    "\n",
    "# Split the data into 5 equal parts, using a fixed random seed for repeatability.\n",
    "splitData = ratingsDataFrame.randomSplit([0.2, 0.2, 0.2, 0.2, 0.2], (numberOfFolds*numberOfFolds))\n",
    "\n",
    "# Init blank arrays to store evaluations of the ALS models\n",
    "als1RMSE = []\n",
    "als1MAE = []\n",
    "als2RMSE = []\n",
    "als2MAE = []\n",
    "\n",
    "alsA = ALS(maxIter = 2, regParam = 0.1, userCol = 'userId', itemCol = 'movieId', ratingCol = 'rating',\n",
    "             coldStartStrategy = 'drop')\n",
    "alsB = ALS(maxIter = 10, regParam = 0.1, userCol = 'userId', itemCol = 'movieId', ratingCol = 'rating',\n",
    "             coldStartStrategy = 'drop')\n",
    "\n",
    "rmseAEvaluator = RegressionEvaluator(metricName = 'rmse', labelCol = 'rating', predictionCol = 'prediction')\n",
    "maeAEvaluator = RegressionEvaluator(metricName = 'mae', labelCol = 'rating', predictionCol = 'prediction')\n",
    "rmseBEvaluator = RegressionEvaluator(metricName = 'rmse', labelCol = 'rating', predictionCol = 'prediction')\n",
    "maeBEvaluator = RegressionEvaluator(metricName = 'mae', labelCol = 'rating', predictionCol = 'prediction')\n",
    "\n",
    "# Iterate through each fold\n",
    "for fold in range(numberOfFolds):\n",
    "    print(\"Begin Fold: \" + str(fold + 1))\n",
    "    \n",
    "    # Select the test data from the 5 pre-split data sets\n",
    "    test = splitData[fold]\n",
    "    # Create a blank Data Frame with same schema as test and the original data frame\n",
    "    # will be used to store rest of pre-split data sets\n",
    "    train = spark.createDataFrame(sc.emptyRDD(), ratingsDataFrame.schema)\n",
    "    \n",
    "    # Iterate through each dataset in pre-split datasets\n",
    "    for split in splitData:\n",
    "        # If the dataset isnt the set set then it is part of the training set\n",
    "        if split != test:\n",
    "            train = train.union(split)\n",
    "             \n",
    "    print(\"Fit ALS 1\")\n",
    "    alsAModel = alsA.fit(train)\n",
    "    print(\"Fit ALS 2\")\n",
    "    alsBModel = alsB.fit(train)\n",
    "    \n",
    "    print(\"Begin ALS 1 Evaluation\")\n",
    "    alsAPredictions = alsAModel.transform(test)\n",
    "    alsARMSEEval = rmseAEvaluator.evaluate(alsAPredictions)\n",
    "    alsAMAEEval = maeAEvaluator.evaluate(alsAPredictions)\n",
    "    \n",
    "    print(\"Begin ALS 2 Evaluation\")\n",
    "    alsBPredictions = alsBModel.transform(test)\n",
    "    alsBRMSEEval = rmseBEvaluator.evaluate(alsBPredictions)\n",
    "    alsBMAEEval = maeBEvaluator.evaluate(alsBPredictions)\n",
    "    \n",
    "    print(alsARMSEEval)\n",
    "    print(alsAMAEEval)\n",
    "    print(alsBRMSEEval)\n",
    "    print(alsBMAEEval)\n",
    "    \n",
    "    als1RMSE.append(alsARMSEEval)\n",
    "    als1MAE.append(alsAMAEEval)\n",
    "    als2RMSE.append(alsBRMSEEval)\n",
    "    als2MAE.append(alsBMAEEval)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignment Question 2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.clustering import KMeansModel\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master('local[2]') \\\n",
    "    .appName('Question 2C') \\\n",
    "    .config(\"spark.local.dir\",\"/fastdata/acp18dck\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel('WARN')\n",
    "\n",
    "dfItemFactors = alsAModel.itemFactors\n",
    "df2 = dfItemFactors.select('features')\n",
    "\n",
    "schema = StructType([StructField(str(i), DoubleType(), True) for i in range(10)])\n",
    "correctSchema = spark.createDataFrame(sc.emptyRDD(), schema)\n",
    "\n",
    "# Convert array of features into individual columns of type double as analogous with lab 4 dataframe\n",
    "exploded = df2.select([funcs.col('features').getItem(i) for i in range(10)])\n",
    "correctSchema = exploded\n",
    "\n",
    "# Hits 'NoneType' object has no attribute 'sc' here\n",
    "#exploded.rdd.map(lambda r: [Vectors.dense(r[:-1])]).toDF(exploded, schema)\n",
    "\n",
    "#def transData(data):\n",
    " #   return data.rdd.map(lambda r: [Vectors.dense(r[:-1])]).toDF(['features'])\n",
    "\n",
    "#features = transData(df2)\n",
    "\n",
    "kValue = 20\n",
    "silhouettes = np.zeros(kValue)\n",
    "\n",
    "clusterList = []\n",
    "tagList = []\n",
    "tagSummary = []\n",
    "\n",
    "for k in range(2, kValue):\n",
    "    kmeans = KMeans().setK(k).setSeed(11)\n",
    "    model = kmeans.fit(features)\n",
    "    \n",
    "    summary = model.summary\n",
    "    clusterSize = summary.clusterSizes\n",
    "    topClusterSize = heapq.nlargest(3, clusterSize)\n",
    "    \n",
    "    predictions = model.transform(features)\n",
    "    \n",
    "    movieIDCluster = model.itemFactors.withColumn(\"index\", monotonically_increasing_id()).join(predictions.withColumn(\"index\", monotonically_increasing_id()), on=['index'], how = 'left_outer').orderBy(\"index\")\n",
    "    movieIDCluster = movieIDCluster.selectExpr(\"index as index\", \"id as movieId\", \"prediction as cluster\")\n",
    "    \n",
    "    movieIDClusterCount = movieIDCluster.groupby(\"cluster\").count().orderBy(col(\"count\").desc())  \n",
    "    top3Clusters = movieIDClusterCount.limit(3)\n",
    "    top3ClustersList = top3Clusters.select(\"cluster\").rdd.flatMap(lambda x: x).collect()\n",
    "    clusterList.append(top3ClustersList)\n",
    "    print(\"Split:\", i+1, \" top 3 clusters:\", top3ClustersList, \" each cluster size:\", topClusterSize)\n",
    "    \n",
    "    movieIDTagID = newMovieTagID.join(newTagIDTag, on =['tagID'],how ='left_outer')\n",
    "    movieIDClusterTag = movieIDCluster.join(movieIDTagID, on = ['movieId'], how = 'left_outer')\n",
    "    \n",
    "    for j in range(3):\n",
    "        rankTags = movieIDClusterTag.filter(col(\"cluster\") == top3ClustersList[j]).groupBy(\"tag\").sum('relevance').na.drop().orderBy(col(\"sum(relevance)\").desc())\n",
    "        top5Tags = rankTags.limit(5).select(\"tag\").rdd.flatMap(lambda x: x).collect()  \n",
    "        tagList.append(top5_tag)    \n",
    "        print(\"For cluster:\", top3ClustersList[j], \" top 5 tags:\", top5Tags)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python (36)",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
